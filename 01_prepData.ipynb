{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates all the bootleg score fragments and data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import glob\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse valid files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we loop through all the labeled files and create a dictionary with the piece id (e.g. violin_1) as keys and a list of the corresponding valid pages in that piece as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile(fname):\n",
    "    \"\"\"\n",
    "    Parse a text file with file ids and valid pages and create a dictionary mapping the two.\n",
    "    For example, one element in the dictionary would be in the form {\"violin_1\": [1, 2, 3]}.\n",
    "    \"\"\"\n",
    "    valid_dict = {}\n",
    "    with open (fname, 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(\" \")\n",
    "            ID = data[0]\n",
    "            valid_pages = removeFiller(data)\n",
    "            if valid_pages != []:\n",
    "                valid_dict[ID] = valid_pages\n",
    "    return valid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeFiller(data):\n",
    "    \"\"\"\n",
    "    Takes in a page range in the form \"1-3, 5-6\" and returns a list containing each valid page, e.g. [1, 2, 3, 5, 6].\n",
    "    \"\"\"\n",
    "    if len(data) == 1:\n",
    "        return []\n",
    "    ranges = data[1:]\n",
    "    valid_pages = []\n",
    "    for r in ranges:\n",
    "        num_range = r.replace(',',\"\")\n",
    "        if '-' in num_range:\n",
    "            left = int(num_range.split(\"-\")[0])\n",
    "            right = int(num_range.split(\"-\")[1])\n",
    "            valid_pages.extend(list(np.arange(left,right+1)))\n",
    "        else:\n",
    "            page = int(num_range)\n",
    "            valid_pages.append(page)\n",
    "    return valid_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = parseFile('cfg_files/labeled_annotations.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we split our labeled data into train, valid, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainValidTest(IDs, train=.6, validation=.2, test=.2, savefile = None):\n",
    "    \"\"\"\n",
    "    Splits a list of IDs into 60% train, 20% validation, 20% test.\n",
    "    \"\"\"\n",
    "    random.shuffle(IDs)\n",
    "    length = len(IDs)\n",
    "    train = IDs[:int(.6*length)]\n",
    "    valid = IDs[int(.6*length):int(.8*length)]\n",
    "    test = IDs[int(.8*length):]\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled_IDs = list(labeled_data.keys())\n",
    "train_ids, valid_ids, test_ids = splitTrainValidTest(labeled_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cfg_files/test_files.txt\", 'w') as f:\n",
    "    for i in test_ids:\n",
    "        f.write(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Balanced Fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we sample a fixed number of fragments from each instrument to generate a balanced number of fragments for each instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_bscore(ID, pages, root = '../bootleg_data-v1/labeled/'):\n",
    "    \"\"\"\n",
    "    Takes in an ID, list of valid pages, and the directory of the labeled data \n",
    "    and concatenates all the bootleg scores from those pages together into one long bootleg score.\n",
    "    \"\"\"\n",
    "    instrument = ID.split(\"_\")[0]\n",
    "    fname = os.path.join(root, instrument, ID+'.bscore')\n",
    "    global_bscore = []\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        for page in pages:\n",
    "            try:\n",
    "                global_bscore.extend(data[page-1])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(fname)\n",
    "                print(page)\n",
    "                print(len(data))\n",
    "                print(data)\n",
    "                sys.exit(1)\n",
    "    return global_bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colToString(s):     \n",
    "    str1 = \"\"  \n",
    "    for ele in s:  \n",
    "        str1 += str(int(ele))+\" \"     \n",
    "    return str1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_balance(ID_list, labeled_data, fragment_size, samplesPerInstrument):\n",
    "    \"\"\"\n",
    "    Takes in a list of IDs and the labeled_data dictionary containing a mapping between IDs and \n",
    "    valid pages and samples samplesPerInstrument fragments with length fragment_size from \n",
    "    each instrument.\n",
    "    \"\"\"\n",
    "    count = {}\n",
    "    fragments = {}\n",
    "    \n",
    "    for ID in ID_list:\n",
    "        instrument = ID.split(\"_\")[0]\n",
    "        if instrument not in fragments.keys():\n",
    "            fragments[instrument] = []\n",
    "            count[instrument] = 0\n",
    "        bootleg_score = get_global_bscore(ID, labeled_data[ID])\n",
    "        valid_locations = np.arange(0,len(bootleg_score) - fragment_size+1)\n",
    "        for loc in valid_locations:\n",
    "            fragments[instrument].append((ID, bootleg_score[loc:loc+fragment_size]))\n",
    "            count[instrument]+=1\n",
    "\n",
    "    db_labels = []\n",
    "    db_fragments = []\n",
    "    for instrument in fragments.keys():\n",
    "        all_fragments = list(np.arange(0,count[instrument]))\n",
    "        data = sorted(random.choices(all_fragments, k=samplesPerInstrument))\n",
    "        for i in data:\n",
    "            ID, fragment = fragments[instrument][i]\n",
    "            db_labels.append(instrument)\n",
    "            db_fragments.append(colToString(fragment))\n",
    "    df = pd.DataFrame({\"Instrument\":db_labels,\"Fragment\":db_fragments})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFragments(train, valid, test, fragment_size, samplesPerInstrument):\n",
    "    train_df = class_balance(train, labeled_data, fragment_size, samplesPerInstrument)\n",
    "    valid_df = class_balance(valid, labeled_data, fragment_size, samplesPerInstrument//3)\n",
    "    test_df = class_balance(test, labeled_data, fragment_size, samplesPerInstrument//3)\n",
    "    train_df.to_csv(f\"train_df-frag{fragment_size}.csv\", index = False)\n",
    "    valid_df.to_csv(f\"valid_df-frag{fragment_size}.csv\", index = False)\n",
    "    test_df.to_csv(f\"test_df-frag{fragment_size}.csv\", index = False)\n",
    "    return train_df, valid_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fragment_size = 64 \n",
    "samplesPerInstrument = 3600\n",
    "train, valid, test = getFragments(train_ids, valid_ids, test_ids, fragment_size, samplesPerInstrument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the train.csv, valid.csv, and test.csv files for the proxy classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootlegCSVFiles(infile_train, infile_valid, infile_test, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates train.csv and test.csv from bootleg score fragments.\n",
    "    '''\n",
    "    train_df = pd.read_csv(\"train_df-frag64.csv\")\n",
    "    valid_df = pd.read_csv(\"valid_df-frag64.csv\")\n",
    "    test_df = pd.read_csv(\"test_df-frag64.csv\")\n",
    "    train_df = train_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    valid_df = valid_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    test_df = test_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    \n",
    "    for df, outfile in [(train_df, outfile_train), (valid_df, outfile_valid), (test_df, outfile_test)]:\n",
    "        # convert each row of bootleg scores to decimal\n",
    "        df[\"text\"] = df[\"text\"].apply(lambda x: ''.join(x.split(',')))\n",
    "        df.to_csv(outfile, index = False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_infile = \"data/train_df-frag64.csv\"\n",
    "test_infile = \"data/test_df-frag64.csv\"\n",
    "valid_infile = \"data/valid_df-frag64.csv\"\n",
    "csv_train_file = path/'train64.csv'\n",
    "csv_valid_file = path/'valid64.csv'\n",
    "csv_test_file = path/'test64.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateBootlegCSVFiles(train_infile, valid_infile, test_infile, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate csv files for evaluating on the original page classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSVFiles(train_ids, valid_ids, test_ids, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates csv files for the original task of classifying full pages of music.\n",
    "    '''\n",
    "    \n",
    "    generateFullPageCSV(train_ids, outfile_train)\n",
    "    generateFullPageCSV(valid_ids, outfile_valid)\n",
    "    generateFullPageCSV(test_ids, outfile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSV(id_list, outfile, root = '/home/kji/InstrumentID/bootleg_data-v1/labeled/'):\n",
    "    \n",
    "    with open(outfile, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for id_string in id_list: # e.g. violin_1\n",
    "            instrument = id_string.split(\"_\")[0]\n",
    "            fname = os.path.join(root, instrument, id_string+'.bscore')\n",
    "            with open(fname, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for m in data:\n",
    "                    if m:\n",
    "                        textStr = ' '.join([str(i) for i in m])\n",
    "                        fout.write(f'{instrument},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train_file = path/'train.fullpage.csv'\n",
    "csv_valid_file = path/'valid.fullpage.csv'\n",
    "csv_test_file = path/'test.fullpage.csv'\n",
    "generateFullPageCSVFiles(train_ids, valid_ids, test_ids, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate CSV files for averaging the predictions on multiple fixed-length windows of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnsembleCSV(id_list, chunkSz, outfile_test, root = '/home/kji/InstrumentID/bootleg_data-v1/labeled/'):\n",
    "    '''\n",
    "    Generates a csv file to facilitate evaluating fixed-length classifiers on the full page classification task.\n",
    "    Each line in the file corresponds to a fixed-length window of samples within a page.  The predictions from\n",
    "    all windows within a single page can then be averaged and evaluated.\n",
    "    '''\n",
    "    with open(outfile_test, 'w') as fout:\n",
    "        fout.write('id,label,text\\n')\n",
    "        for pieceID in id_list:\n",
    "            instrument = pieceID.split(\"_\")[0]\n",
    "            fname = os.path.join(root, instrument, pieceID+'.bscore')\n",
    "            with open(fname, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for i, page in enumerate(labeled_data[pieceID]):\n",
    "                    if data[page-1] and len(data[page-1]) > 0:\n",
    "                        if len(data[page-1]) < chunkSz:  # only 1 window\n",
    "                            ints = data[page-1]\n",
    "                            textStr = ' '.join([str(i) for i in ints])\n",
    "                            idString = f'{pieceID}_{i}_0'\n",
    "                            fout.write(f'{idString},{instrument},{textStr}\\n')\n",
    "                        else: # multiple windows\n",
    "                            numWindows = int(np.ceil(len(data[page-1])/(chunkSz/2))) - 1 # hop by half the chunk size\n",
    "                            for j in range(numWindows - 1):\n",
    "                                startIdx = chunkSz // 2 * j\n",
    "                                endIdx = startIdx + chunkSz\n",
    "                                ints = data[page-1][startIdx: endIdx]\n",
    "                                textStr = ' '.join([str(i) for i in ints])\n",
    "                                idString = f'{pieceID}_{i}_0'\n",
    "                                fout.write(f'{idString},{instrument},{textStr}\\n')\n",
    "                            # handle last window\n",
    "                            ints = data[page-1][-chunkSz:]\n",
    "                            textStr = ' '.join([str(i) for i in ints])\n",
    "                            idString = f'{pieceID}_{i}_{numWindows-1}' \n",
    "                            fout.write(f'{idString},{instrument},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateEnsembleCSV(test_ids, chunkSz, path/'test.ensemble256.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below, we will prepare the data for use with the fastai library.  This is adapted from the fast.ai [ULMFit tutorial](https://github.com/fastai/course-nlp/blob/master/nn-vietnamese.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Config.data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'solo_bscore_lm'\n",
    "path = data_path/name\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Language Model Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the bootleg score features into string representations of decimal integers.  Generate one document per pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_target = path/'solo_target'\n",
    "path_target.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootlegStringFiles(bscore_feats_dir, outdir):\n",
    "    '''\n",
    "    Converts the bootleg score features to string decimal representation, and writes them\n",
    "    to text files in the specified directory.\n",
    "    '''    \n",
    "    # e.g. /home/dyang/InstrumentID/bootleg_data-v1/labeled/violin/violin_1.bscore\n",
    "    for pieceDir in bscore_feats_dir.rglob(\"*.bscore\"): \n",
    "        label = pieceDir.parts[-1].split('.')[0]\n",
    "        outfile = outdir/(label + '.txt')\n",
    "        with open(outfile,'w') as fout:\n",
    "            with open(pieceDir, 'rb') as fin:\n",
    "                data = pickle.load(fin)\n",
    "                for ints in data:\n",
    "                    if ints:\n",
    "                        pageStr = ' '.join([str(i) for i in ints]) \n",
    "                        fout.write(pageStr)\n",
    "#                         fout.write('\\n\\n')\n",
    "                fout.write('</doc>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscore_dir = Path('/home/kji/InstrumentID/bootleg_data-v1/labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateBootlegStringFiles(bscore_dir, path_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/kji/.fastai/data/solo_bscore_lm/solo_target')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "basicTokenizer = Tokenizer(pre_rules=[], post_rules=[])\n",
    "lm_target_data = (TextList.from_folder(path_target, processor=[OpenFileProcessor(), TokenizeProcessor(tokenizer=basicTokenizer), NumericalizeProcessor()])\n",
    "            .split_by_rand_pct(0.1, seed=42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "lm_target_data.save(path/'solo_lm_target_databunch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Solo Music Language Model Databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but using the bootleg score dataset for all solo pieces for the set of instruments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all = path/'solo_all'\n",
    "path_all.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bscore_dir = Path('/home/kji/InstrumentID/bootleg_data-v1/all/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateBootlegStringFiles(bscore_dir, path_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the train.csv, valid.csv, and test.csv files for the proxy classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBootlegCSVFiles(infile_train, infile_valid, infile_test, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates train.csv and test.csv from bootleg score fragments.\n",
    "    '''\n",
    "    train_df = pd.read_csv(\"train_df-frag64.csv\")\n",
    "    valid_df = pd.read_csv(\"valid_df-frag64.csv\")\n",
    "    test_df = pd.read_csv(\"test_df-frag64.csv\")\n",
    "    train_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    valid_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    test_df.rename(columns={\"Instrument\": \"label\", \"Fragment\": \"text\"})\n",
    "    \n",
    "    for df, outfile in [(train_df, outfile_train), (valid_df, outfile_valid), (test_df, outfile_test)]:\n",
    "        # convert each row of bootleg scores to decimal\n",
    "        df.text = df.text.map(lambda x: ' 'join([str(i) for i in convertBinaryToInt(x)]))\n",
    "        df.to_csv(outfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_infile = \"train_df-frag64.csv\"\n",
    "test_infile = \"test_df-frag64.csv\"\n",
    "valid_infile = \"valid_df-frag64.csv\"\n",
    "csv_train_file = path/'train64.csv'\n",
    "csv_valid_file = path/'valid64.csv'\n",
    "csv_test_file = path/'test64.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateBootlegCSVFiles(train_infile, valid_infile, test_infile, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate csv files for evaluating on the original page classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSVFiles(train_ids, valid_ids, test_ids, outfile_train, outfile_valid, outfile_test):\n",
    "    '''\n",
    "    Generates csv files for the original task of classifying full pages of music.\n",
    "    '''\n",
    "    \n",
    "    generateFullPageCSV(train_ids, outfile_train)\n",
    "    generateFullPageCSV(valid_ids, outfile_valid)\n",
    "    generateFullPageCSV(test_ids, outfile_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSV(id_list, outfile):\n",
    "    \n",
    "    with open(outfile, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for ID in id_list: # e.g. violin_1\n",
    "            instrument = ID.split(\"_\")[0]\n",
    "            fname = os.path.join(root, instrument, ID+'.bscore')\n",
    "            with open(fname, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for m in data:\n",
    "                    if m:\n",
    "                        ints = convertBinaryToInt(m)\n",
    "                        textStr = ' '.join([str(i) for i in ints])\n",
    "                        fout.write(f'{instrument},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_train_file = path/'train.fullpage.csv'\n",
    "csv_valid_file = path/'valid.fullpage.csv'\n",
    "csv_test_file = path/'test.fullpage.csv'\n",
    "generateFullPageCSVFiles(train, valid, test, csv_train_file, csv_valid_file, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not doing ensembling this week\n",
    "Finally, we also generate csv files to facilitate evaluating fixed-length classifiers on the full page classification task.  These classifiers will be applied to multiple windows of features, and the predictions will be averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEnsembleCSV(test_ids, chunkSz, outfile_test, root = '/home/kji/InstrumentID/bootleg_data-v1/labeled/'):\n",
    "    '''\n",
    "    Generates a csv file to facilitate evaluating fixed-length classifiers on the full page classification task.\n",
    "    Each line in the file corresponds to a fixed-length window of samples within a page.  The predictions from\n",
    "    all windows within a single page can then be averaged and evaluated.\n",
    "    '''\n",
    "        \n",
    "    with open(outfile_test, 'w') as fout:\n",
    "        fout.write('id,label,text\\n')\n",
    "        for pieceID in test_ids:\n",
    "            instrument = pieceID.split(\"_\")[0]\n",
    "            fname = os.path.join(root, instrument, pieceID+'.bscore')\n",
    "            with open(fname, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                for page in labeled_data[pieceID]:\n",
    "                    if data[page-1] and len(data[page-1]) > 0:\n",
    "                        ints = data[page-1]\n",
    "                        print(ints)\n",
    "            for i, m in enumerate(d[pieceDir]): # d[pieceDir] -> list of binary bootleg score matrices, one per page\n",
    "                if m is not None and m.shape[1] > 0:\n",
    "                    if m.shape[1] <= chunkSz: # only 1 window\n",
    "                        ints = convertBinaryToInt(m)\n",
    "                        textStr = ' '.join([str(i) for i in ints])\n",
    "                        idString = f'{pieceID}_{i}_0' # id: pieceID_pageIdx_chunkIdx\n",
    "                        fout.write(f'{idString},{composer},{textStr}\\n')\n",
    "                    else: # multiple windows\n",
    "                        numWindows = int(np.ceil(m.shape[1]/(chunkSz/2))) - 1 # hop by half the chunk size\n",
    "                        for j in range(numWindows - 1):\n",
    "                            startIdx = chunkSz // 2 * j\n",
    "                            endIdx = startIdx + chunkSz\n",
    "                            ints = convertBinaryToInt(m[:,startIdx:endIdx])\n",
    "                            textStr = ' '.join([str(i) for i in ints])\n",
    "                            idString = f'{pieceID}_{i}_{j}' # id: pieceID_pageIdx_chunkIdx\n",
    "                            fout.write(f'{idString},{composer},{textStr}\\n')\n",
    "                        # handle last window\n",
    "                        ints = convertBinaryToInt(m[:,-chunkSz:])\n",
    "                        textStr = ' '.join([str(i) for i in ints])\n",
    "                        idString = f'{pieceID}_{i}_{numWindows-1}' \n",
    "                        fout.write(f'{idString},{composer},{textStr}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_test_file = path/'test.ensemble256.csv'\n",
    "generateEnsembleCSV(save_pages_file, 256, csv_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and testing the Transformer-based models.  Instead of using decimal string representations, we represent each 62-bit bootleg score feature as a sequence of 8 one-byte characters.  Rather than generating these from scratch, we will simply convert the existing files to the new format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateLMTrainFiles(indir, out_train, out_valid, val_frac=0.1):\n",
    "    \n",
    "    # split train/validation by file\n",
    "    filelist = sorted(glob.glob('{}/*.txt'.format(indir)))\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(filelist)\n",
    "    endIdx = int(len(filelist) * (1-val_frac)) + 1\n",
    "    train_files = filelist[0:endIdx]\n",
    "    valid_files = filelist[endIdx:]\n",
    "    \n",
    "    # convert to binary string representation\n",
    "    convertToByteChars(train_files, out_train)\n",
    "    convertToByteChars(valid_files, out_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToByteChars(filelist, outfile):\n",
    "    '''\n",
    "    Split each 62-bit bootleg score feature into 8 bytes, and express each byte as a single character.\n",
    "    Consecutive bootleg score feature `words' will be separated by space.\n",
    "    '''\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for infile in filelist:\n",
    "            with open(infile, 'r') as fin:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if len(line) > 0:\n",
    "                        if line == '</doc>':\n",
    "                            pass # skip\n",
    "                        else:\n",
    "                            converted = convertLineToCharSeq(line)\n",
    "                            fout.write(f'{converted}\\n')\n",
    "            fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLineToCharSeq(line):\n",
    "    ints = [int(p) for p in line.split()]\n",
    "    result = ' '.join([int2charseq(i) for i in ints])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2charseq(int64):\n",
    "    chars = ''\n",
    "    for i in range(8):\n",
    "        numshift = i * 8\n",
    "        charidx = (int64 >> numshift) & 255\n",
    "        chars += chr(19968 + charidx) # 19968 ensures that all chars are chinese characters (not newline, space, etc)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = path/'bpe_data'\n",
    "bpe_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target data\n",
    "lm_train_file = bpe_path/'bpe_lm_target_train.txt'\n",
    "lm_valid_file = bpe_path/'bpe_lm_target_valid.txt'\n",
    "dir_to_convert = path/'solo_target'\n",
    "generateLMTrainFiles(dir_to_convert, lm_train_file, lm_valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IMSLP data\n",
    "lm_train_file = bpe_path/'bpe_lm_all_train.txt'\n",
    "lm_valid_file = bpe_path/'bpe_lm_all_valid.txt'\n",
    "dir_to_convert = path/'solo_all'\n",
    "generateLMTrainFiles(dir_to_convert, lm_train_file, lm_valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSingleCSVFile(infile, outfile):\n",
    "    '''\n",
    "    Convert .csv file with decimal string representation of bootleg score features to\n",
    "    a .csv file with byte character representation.\n",
    "    '''\n",
    "    with open(infile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i==0: \n",
    "                fout.write(line) # header\n",
    "            else:\n",
    "                parts = line.strip().split(',')\n",
    "                feats = parts.pop()\n",
    "                charseq = convertLineToCharSeq(feats)\n",
    "                strToWrite = ','.join(parts) + ',' + charseq + '\\n'\n",
    "                fout.write(strToWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertAllCSVFiles(indir, outdir):\n",
    "    assert indir != outdir\n",
    "    os.makedirs(outdir, exist_ok = True)\n",
    "    for infile in glob.glob(f'{indir}/*.csv'):\n",
    "        print(f'Converting {os.path.basename(infile)}')\n",
    "        basename = os.path.splitext(os.path.basename(infile))[0]\n",
    "        outfile = f'{outdir}/{basename}.char.csv'\n",
    "        convertSingleCSVFile(infile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertAllCSVFiles(str(path), str(bpe_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
